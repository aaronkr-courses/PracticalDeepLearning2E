{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02290733",
   "metadata": {},
   "source": [
    "\n",
    "# Fashion MNIST Neural Network Comprehensive Experiments\n",
    "\n",
    "- This notebook consolidates all experiments into a single workflow\n",
    "- Designed to run on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c11e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b79b3",
   "metadata": {},
   "source": [
    "## Load Fashion MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beff663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. LOAD FASHION MNIST DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading Fashion MNIST dataset...\")\n",
    "# Load from keras/tensorflow which has Fashion MNIST built-in\n",
    "from tensorflow import keras\n",
    "\n",
    "(x_train_full, y_train_full), (x_test_full, y_test_full) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Flatten images and normalize to [0, 1]\n",
    "x_train_full = x_train_full.reshape(-1, 784).astype('float64') / 255.0\n",
    "x_test_full = x_test_full.reshape(-1, 784).astype('float64') / 255.0\n",
    "\n",
    "print(f\"Training set shape: {x_train_full.shape}\")\n",
    "print(f\"Test set shape: {x_test_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a85b70",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b379fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def count_params(clf):\n",
    "    \"\"\"Count total parameters in the network\"\"\"\n",
    "    params = 0\n",
    "    for w in clf.coefs_:\n",
    "        params += w.shape[0] * w.shape[1]\n",
    "    for b in clf.intercepts_:\n",
    "        params += b.shape[0]\n",
    "    return params\n",
    "\n",
    "def train_and_evaluate(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Train model and return score, loss, params, time\"\"\"\n",
    "    s = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    e = time.time() - s\n",
    "    loss = clf.loss_\n",
    "    params = count_params(clf)\n",
    "    score = clf.score(x_test, y_test)\n",
    "    return score, loss, params, e\n",
    "\n",
    "def train_epochwise(x_train, y_train, x_test, y_test, clf, max_epochs):\n",
    "    \"\"\"Train one epoch at a time and track metrics\"\"\"\n",
    "    train_loss = []\n",
    "    train_err = []\n",
    "    val_err = []\n",
    "    \n",
    "    clf.max_iter = 1\n",
    "    clf.warm_start = True\n",
    "    \n",
    "    for i in range(max_epochs):\n",
    "        clf.fit(x_train, y_train)\n",
    "        train_loss.append(clf.loss_)\n",
    "        train_err.append(1.0 - clf.score(x_train, y_train))\n",
    "        val_err.append(1.0 - clf.score(x_test, y_test))\n",
    "        if (i + 1) % max(1, max_epochs // 10) == 0:\n",
    "            print(f\"  Epoch {i+1}/{max_epochs}: val_err = {val_err[-1]:.5f}\")\n",
    "    \n",
    "    return train_loss, train_err, val_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285044f9",
   "metadata": {},
   "source": [
    "## Experiment 1: Basic Training with Different Architectures and Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT 1: BASIC TRAINING WITH DIFFERENT ARCHITECTURES & ACTIVATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 1: BASIC TRAINING - Architecture & Activation Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "N = 5000  # Use subset for faster training\n",
    "x_train = x_train_full[:N]\n",
    "y_train = y_train_full[:N]\n",
    "x_test = x_test_full[:N]\n",
    "y_test = y_test_full[:N]\n",
    "\n",
    "layer_configs = [\n",
    "    (1,), (500,), (800,), (1000,), (2000,),\n",
    "    (1000, 500), (3000, 1500),\n",
    "    (2, 2, 2), (1000, 500, 250), (2000, 1000, 500),\n",
    "]\n",
    "\n",
    "activation_functions = [\"relu\", \"logistic\", \"tanh\"]\n",
    "\n",
    "results_basic = {act: [] for act in activation_functions}\n",
    "\n",
    "for act in activation_functions:\n",
    "    print(f\"\\n{act.upper()}:\")\n",
    "    for layer in layer_configs:\n",
    "        scores = []\n",
    "        losses = []\n",
    "        times = []\n",
    "        \n",
    "        for i in range(3):  # 3 runs for averaging\n",
    "            clf = MLPClassifier(\n",
    "                solver=\"sgd\", verbose=False, tol=1e-8,\n",
    "                nesterovs_momentum=False, early_stopping=False,\n",
    "                learning_rate_init=0.001, momentum=0.9, max_iter=200,\n",
    "                hidden_layer_sizes=layer, activation=act\n",
    "            )\n",
    "            score, loss, params, elapsed = train_and_evaluate(\n",
    "                x_train, y_train, x_test, y_test, clf\n",
    "            )\n",
    "            scores.append(score)\n",
    "            losses.append(loss)\n",
    "            times.append(elapsed)\n",
    "        \n",
    "        s_arr = np.array(scores)\n",
    "        l_arr = np.array(losses)\n",
    "        t_arr = np.array(times)\n",
    "        se = s_arr.std() / np.sqrt(len(scores))\n",
    "        le = l_arr.std() / np.sqrt(len(losses))\n",
    "        \n",
    "        results_basic[act].append({\n",
    "            'layers': layer, 'score': s_arr.mean(), 'score_err': se,\n",
    "            'loss': l_arr.mean(), 'loss_err': le, 'params': params,\n",
    "            'time': t_arr.mean()\n",
    "        })\n",
    "        \n",
    "        print(f\"  layers: {str(layer):20s} | score: {s_arr.mean():.4f} ± {se:.4f} | \"\n",
    "              f\"loss: {l_arr.mean():.4f} ± {le:.4f} | params: {params:6d} | time: {t_arr.mean():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e92124",
   "metadata": {},
   "source": [
    "## Experiment 2: Batch Size Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT 2: BATCH SIZE EFFECTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 2: BATCH SIZE EFFECTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "N = 16384\n",
    "x_train = x_train_full[:N]\n",
    "y_train = y_train_full[:N]\n",
    "x_test = x_test_full[:N]\n",
    "y_test = y_test_full[:N]\n",
    "\n",
    "batch_sizes = [16384, 8192, 4096, 2048, 1024, 512, 256, 128, 64, 32, 16, 8, 4, 2]\n",
    "M = 8192  # Total minibatches to keep constant\n",
    "\n",
    "batch_results = {'batch_size': [], 'score': [], 'score_err': [], 'loss': [], 'loss_err': []}\n",
    "\n",
    "for bz in batch_sizes:\n",
    "    print(f\"\\nbatch_size = {bz}:\")\n",
    "    epochs = (M * bz) // N\n",
    "    if epochs < 1:\n",
    "        epochs = 1\n",
    "    \n",
    "    scores = []\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        clf = MLPClassifier(\n",
    "            solver=\"sgd\", verbose=False, tol=1e-8,\n",
    "            nesterovs_momentum=False, early_stopping=False,\n",
    "            learning_rate_init=0.001, momentum=0.9, max_iter=epochs,\n",
    "            hidden_layer_sizes=(1000, 500), activation=\"relu\",\n",
    "            batch_size=bz\n",
    "        )\n",
    "        score, loss, params, _ = train_and_evaluate(\n",
    "            x_train, y_train, x_test, y_test, clf\n",
    "        )\n",
    "        scores.append(score)\n",
    "        losses.append(loss)\n",
    "        print(f\"  Run {i+1}: score = {score:.5f}, loss = {loss:.5f}\")\n",
    "    \n",
    "    s_arr = np.array(scores)\n",
    "    l_arr = np.array(losses)\n",
    "    se = s_arr.std() / np.sqrt(len(scores))\n",
    "    le = l_arr.std() / np.sqrt(len(losses))\n",
    "    \n",
    "    batch_results['batch_size'].append(bz)\n",
    "    batch_results['score'].append(s_arr.mean())\n",
    "    batch_results['score_err'].append(se)\n",
    "    batch_results['loss'].append(l_arr.mean())\n",
    "    batch_results['loss_err'].append(le)\n",
    "    \n",
    "    print(f\"  Mean: score = {s_arr.mean():.5f} ± {se:.5f}, loss = {l_arr.mean():.5f} ± {le:.5f}\")\n",
    "\n",
    "# Plot batch size results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax1.errorbar(batch_results['batch_size'], batch_results['score'], \n",
    "             batch_results['score_err'], marker='o', color='k', fillstyle='none')\n",
    "ax1.set_xlabel('Minibatch Size', fontsize=12)\n",
    "ax1.set_ylabel('Test Score', fontsize=12)\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.errorbar(batch_results['batch_size'], batch_results['loss'], \n",
    "             batch_results['loss_err'], marker='s', color='k', fillstyle='none')\n",
    "ax2.set_xlabel('Minibatch Size', fontsize=12)\n",
    "ax2.set_ylabel('Training Loss', fontsize=12)\n",
    "ax2.set_xscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('batch_size_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60118ff5",
   "metadata": {},
   "source": [
    "## Experiment 3: Base Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT 3: BASE LEARNING RATE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 3: BASE LEARNING RATE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "N = 10000\n",
    "x_train = x_train_full[:N]\n",
    "y_train = y_train_full[:N]\n",
    "x_test = x_test_full[:N]\n",
    "y_test = y_test_full[:N]\n",
    "\n",
    "base_lrs = [0.2, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "lr_results = {'lr': [], 'score_fixed': [], 'score_scaled': [], 'time_scaled': []}\n",
    "\n",
    "print(\"\\nFixed epochs (50):\")\n",
    "for lr in base_lrs:\n",
    "    clf = MLPClassifier(\n",
    "        solver=\"sgd\", verbose=False, tol=1e-8,\n",
    "        nesterovs_momentum=False, early_stopping=False,\n",
    "        learning_rate_init=lr, momentum=0.9, max_iter=50,\n",
    "        hidden_layer_sizes=(1000, 500), activation=\"relu\",\n",
    "        learning_rate=\"constant\", batch_size=64\n",
    "    )\n",
    "    score, loss, _, _ = train_and_evaluate(\n",
    "        x_train, y_train, x_test, y_test, clf\n",
    "    )\n",
    "    lr_results['lr'].append(lr)\n",
    "    lr_results['score_fixed'].append(score)\n",
    "    print(f\"  lr = {lr:.5f}: score = {score:.5f}, loss = {loss:.5f}\")\n",
    "\n",
    "print(\"\\nScaled epochs (lr * epochs ≈ 1.5):\")\n",
    "epochs_list = [8, 15, 30, 150, 300, 1500, 3000, 15000]\n",
    "score_scaled = []\n",
    "time_scaled = []\n",
    "\n",
    "for i, lr in enumerate(base_lrs):\n",
    "    clf = MLPClassifier(\n",
    "        solver=\"sgd\", verbose=False, tol=1e-8,\n",
    "        nesterovs_momentum=False, early_stopping=False,\n",
    "        learning_rate_init=lr, momentum=0.9, max_iter=epochs_list[i],\n",
    "        hidden_layer_sizes=(1000, 500), activation=\"relu\",\n",
    "        learning_rate=\"constant\", batch_size=64\n",
    "    )\n",
    "    score, loss, _, elapsed = train_and_evaluate(\n",
    "        x_train, y_train, x_test, y_test, clf\n",
    "    )\n",
    "    score_scaled.append(score)\n",
    "    time_scaled.append(elapsed)\n",
    "    print(f\"  lr = {lr:.5f} (epochs={epochs_list[i]}): score = {score:.5f}, time = {elapsed:.2f}s\")\n",
    "\n",
    "lr_results['score_scaled'] = score_scaled\n",
    "lr_results['time_scaled'] = time_scaled\n",
    "\n",
    "# Plot learning rate results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.semilogx(lr_results['lr'], lr_results['score_fixed'], marker='o', \n",
    "            label='Fixed epochs (50)', color='k', fillstyle='none')\n",
    "ax.semilogx(lr_results['lr'], lr_results['score_scaled'], marker='s', \n",
    "            label='Scaled epochs (lr×epoch≈1.5)', color='k', fillstyle='none')\n",
    "ax.set_xlabel('Learning Rate (η)', fontsize=12)\n",
    "ax.set_ylabel('Test Score', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_rate_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30aef58",
   "metadata": {},
   "source": [
    "## Experiment 4: Training Set Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20947380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT 4: TRAINING SET SIZE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 4: TRAINING SET SIZE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "x_test = x_test_full[:10000]\n",
    "y_test = y_test_full[:10000]\n",
    "\n",
    "train_sizes = [100, 200, 300, 500, 750, 1000, 1500, 2000, 3000, 5000, 7500, 10000]\n",
    "size_results = {'sizes': [], 'scores': [], 'scores_err': []}\n",
    "\n",
    "for n in train_sizes:\n",
    "    print(f\"\\nTraining set size = {n}\")\n",
    "    x_train = x_train_full[:n]\n",
    "    y_train = y_train_full[:n]\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(3):\n",
    "        epochs = max(1, int((100.0 / n) * 1000))  # ~1000 SGD steps\n",
    "        clf = MLPClassifier(\n",
    "            solver=\"sgd\", verbose=False, tol=1e-8,\n",
    "            nesterovs_momentum=False, early_stopping=False,\n",
    "            learning_rate_init=0.05, momentum=0.9, max_iter=epochs,\n",
    "            hidden_layer_sizes=(1000, 500), activation=\"relu\",\n",
    "            learning_rate=\"constant\", batch_size=100\n",
    "        )\n",
    "        score, loss, _, _ = train_and_evaluate(\n",
    "            x_train, y_train, x_test, y_test, clf\n",
    "        )\n",
    "        scores.append(score)\n",
    "    \n",
    "    s_arr = np.array(scores)\n",
    "    se = s_arr.std() / np.sqrt(len(scores))\n",
    "    size_results['sizes'].append(n)\n",
    "    size_results['scores'].append(s_arr.mean())\n",
    "    size_results['scores_err'].append(se)\n",
    "    print(f\"  Mean score: {s_arr.mean():.5f} ± {se:.5f}\")\n",
    "\n",
    "# Plot training set size results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.errorbar(size_results['sizes'], size_results['scores'], \n",
    "            size_results['scores_err'], marker='o', color='k', fillstyle='none', capsize=5)\n",
    "ax.set_xlabel('Number of Training Samples', fontsize=12)\n",
    "ax.set_ylabel('Test Score', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_size_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fce158",
   "metadata": {},
   "source": [
    "## Experiment 5: L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e905c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT 5: L2 REGULARIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 5: L2 REGULARIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "x_train = x_train_full[:3000]\n",
    "y_train = y_train_full[:3000]\n",
    "x_test = x_test_full[:3000]\n",
    "y_test = y_test_full[:3000]\n",
    "\n",
    "alphas = [0.0, 0.0001, 0.001, 0.01, 0.1]\n",
    "max_epochs = 200\n",
    "l2_results = {alpha: {'val_err': [], 'train_err': []} for alpha in alphas}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, alpha in enumerate(alphas):\n",
    "    print(f\"\\nAlpha (L2) = {alpha}:\")\n",
    "    clf = MLPClassifier(\n",
    "        solver=\"sgd\", verbose=False, tol=0,\n",
    "        nesterovs_momentum=False, early_stopping=False,\n",
    "        learning_rate_init=0.01, momentum=0.0,\n",
    "        hidden_layer_sizes=(100, 50), activation=\"relu\",\n",
    "        alpha=alpha, learning_rate=\"constant\", batch_size=64, max_iter=1\n",
    "    )\n",
    "    \n",
    "    train_loss, train_err, val_err = train_epochwise(\n",
    "        x_train, y_train, x_test, y_test, clf, max_epochs\n",
    "    )\n",
    "    \n",
    "    l2_results[alpha]['train_err'] = train_err\n",
    "    l2_results[alpha]['val_err'] = val_err\n",
    "    \n",
    "    print(f\"  Final: train_err = {train_err[-1]:.5f}, val_err = {val_err[-1]:.5f}\")\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.plot(val_err, linewidth=2, color=f'C{idx}')\n",
    "    ax.set_title(f'Alpha = {alpha}', fontsize=11)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Validation Error')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].remove()\n",
    "plt.tight_layout()\n",
    "plt.savefig('l2_regularization_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d128311",
   "metadata": {},
   "source": [
    "## Experiment 6: Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783e8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT 6: MOMENTUM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 6: MOMENTUM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "x_train = x_train_full[:5000]\n",
    "y_train = y_train_full[:5000]\n",
    "x_test = x_test_full[:5000]\n",
    "y_test = y_test_full[:5000]\n",
    "\n",
    "momentums = [0.0, 0.3, 0.5, 0.7, 0.9, 0.99]\n",
    "max_epochs = 100\n",
    "momentum_results = {m: {'val_err': [], 'train_err': []} for m in momentums}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for idx, m in enumerate(momentums):\n",
    "    print(f\"\\nMomentum = {m}:\")\n",
    "    clf = MLPClassifier(\n",
    "        solver=\"sgd\", verbose=False, tol=0,\n",
    "        nesterovs_momentum=False, early_stopping=False,\n",
    "        learning_rate_init=0.01, momentum=m,\n",
    "        hidden_layer_sizes=(100, 50), activation=\"relu\",\n",
    "        alpha=0.0001, learning_rate=\"constant\", batch_size=64, max_iter=1\n",
    "    )\n",
    "    \n",
    "    train_loss, train_err, val_err = train_epochwise(\n",
    "        x_train, y_train, x_test, y_test, clf, max_epochs\n",
    "    )\n",
    "    \n",
    "    momentum_results[m]['train_err'] = train_err\n",
    "    momentum_results[m]['val_err'] = val_err\n",
    "    \n",
    "    print(f\"  Final: train_err = {train_err[-1]:.5f}, val_err = {val_err[-1]:.5f}\")\n",
    "    \n",
    "    ax.plot(val_err, marker='', linewidth=2, label=f'momentum = {m}', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Validation Error', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('momentum_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ed58c",
   "metadata": {},
   "source": [
    "## Experiment 7: Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT 7: WEIGHT INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 7: WEIGHT INITIALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier as SklearnMLP\n",
    "\n",
    "class CustomMLPClassifier(SklearnMLP):\n",
    "    \"\"\"MLPClassifier with custom weight initialization\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, init_scheme=0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.init_scheme = init_scheme\n",
    "    \n",
    "    def _init_coef(self, fan_in, fan_out, dtype):\n",
    "        if self.init_scheme == 0:\n",
    "            # Glorot (default)\n",
    "            weights, biases = super()._init_coef(fan_in, fan_out, dtype)\n",
    "        elif self.init_scheme == 1:\n",
    "            # Small uniform\n",
    "            weights = 0.01 * (np.random.random((fan_in, fan_out)) - 0.5)\n",
    "            biases = np.zeros(fan_out)\n",
    "        elif self.init_scheme == 2:\n",
    "            # Small Gaussian\n",
    "            weights = 0.005 * np.random.normal(size=(fan_in, fan_out))\n",
    "            biases = np.zeros(fan_out)\n",
    "        elif self.init_scheme == 3:\n",
    "            # He initialization\n",
    "            weights = np.random.normal(size=(fan_in, fan_out)) * np.sqrt(2.0 / fan_in)\n",
    "            biases = np.zeros(fan_out)\n",
    "        elif self.init_scheme == 4:\n",
    "            # Xavier\n",
    "            weights = np.random.normal(size=(fan_in, fan_out)) * np.sqrt(1.0 / fan_in)\n",
    "            biases = np.zeros(fan_out)\n",
    "        \n",
    "        return weights.astype(dtype, copy=False), biases.astype(dtype, copy=False)\n",
    "\n",
    "x_train = x_train_full[:6000]\n",
    "y_train = y_train_full[:6000]\n",
    "x_test = x_test_full[:6000]\n",
    "y_test = y_test_full[:6000]\n",
    "\n",
    "init_names = [\"Glorot\", \"Small Uniform\", \"Small Gaussian\", \"He\", \"Xavier\"]\n",
    "init_schemes = [0, 1, 2, 3, 4]\n",
    "max_epochs = 300\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for scheme_idx, (scheme, name) in enumerate(zip(init_schemes, init_names)):\n",
    "    print(f\"\\nInitialization scheme: {name}\")\n",
    "    test_errs = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        clf = CustomMLPClassifier(\n",
    "            solver=\"sgd\", verbose=False, tol=0,\n",
    "            nesterovs_momentum=False, early_stopping=False,\n",
    "            learning_rate_init=0.01, momentum=0.9,\n",
    "            hidden_layer_sizes=(100, 50), activation=\"relu\",\n",
    "            alpha=0.2, learning_rate=\"constant\", batch_size=64,\n",
    "            max_iter=1, warm_start=(epoch > 0), init_scheme=scheme\n",
    "        )\n",
    "        \n",
    "        clf.fit(x_train, y_train)\n",
    "        test_err = 1.0 - clf.score(x_test, y_test)\n",
    "        test_errs.append(test_err)\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"  Epoch {epoch+1}: test_err = {test_err:.5f}\")\n",
    "    \n",
    "    # Smooth the curve for visualization\n",
    "    window = 21\n",
    "    if len(test_errs) >= window:\n",
    "        smoothed = np.convolve(test_errs, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(smoothed, linewidth=2, label=name, alpha=0.8)\n",
    "    else:\n",
    "        ax.plot(test_errs, linewidth=2, label=name, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Test Error', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('weight_init_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL EXPERIMENTS COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
