{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6d7126",
   "metadata": {},
   "source": [
    "# Huffman Coding Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12cc8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîµ HUFFMAN CODING\")\n",
    "print(\"Adaptive coding - the practical application of information theory!\")\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, char, freq):\n",
    "        self.char = char\n",
    "        self.freq = freq\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.freq < other.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_huffman_tree(text):\n",
    "    \"\"\"Build Huffman tree from text\"\"\"\n",
    "    # Count frequencies\n",
    "    freq_count = Counter(text)\n",
    "    \n",
    "    # Create priority queue (min-heap)\n",
    "    heap = [Node(char, freq) for char, freq in freq_count.items()]\n",
    "    heapq.heapify(heap)\n",
    "    \n",
    "    # Build tree\n",
    "    while len(heap) > 1:\n",
    "        left = heapq.heappop(heap)\n",
    "        right = heapq.heappop(heap)\n",
    "        \n",
    "        merged = Node(None, left.freq + right.freq)\n",
    "        merged.left = left\n",
    "        merged.right = right\n",
    "        \n",
    "        heapq.heappush(heap, merged)\n",
    "    \n",
    "    return heap[0] if heap else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_huffman_codes(root):\n",
    "    \"\"\"Generate Huffman codes from tree\"\"\"\n",
    "    if not root:\n",
    "        return {}\n",
    "    \n",
    "    codes = {}\n",
    "    \n",
    "    def generate_codes_helper(node, code):\n",
    "        if node:\n",
    "            if node.char is not None:  # Leaf node\n",
    "                codes[node.char] = code if code else '0'  # Handle single character case\n",
    "            else:\n",
    "                generate_codes_helper(node.left, code + '0')\n",
    "                generate_codes_helper(node.right, code + '1')\n",
    "    \n",
    "    generate_codes_helper(root, '')\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ab796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_encode(text, codes):\n",
    "    \"\"\"Encode text using Huffman codes\"\"\"\n",
    "    return ''.join(codes[char] for char in text)\n",
    "\n",
    "def calculate_compression_ratio(original_text, encoded_bits):\n",
    "    \"\"\"Calculate compression ratio\"\"\"\n",
    "    original_bits = len(original_text) * 8  # 8 bits per character in ASCII\n",
    "    compressed_bits = len(encoded_bits)\n",
    "    return compressed_bits / original_bits\n",
    "\n",
    "# Demonstrate Huffman coding\n",
    "short_text = \"hello world! this is a test message for huffman coding.\"\n",
    "print(f\"üìù Original text: '{short_text}'\")\n",
    "print(f\"üìè Length: {len(short_text)} characters\")\n",
    "\n",
    "# Build Huffman tree and generate codes\n",
    "root = build_huffman_tree(short_text)\n",
    "huffman_codes = generate_huffman_codes(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be490359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some codes\n",
    "print(\"\\nüî§ Huffman Codes (sample):\")\n",
    "sorted_codes = sorted(huffman_codes.items(), key=lambda x: len(x[1]))\n",
    "for char, code in sorted_codes[:10]:  # Show first 10\n",
    "    char_display = char if char != ' ' else 'SPACE'\n",
    "    print(f\"   '{char_display}': {code}\")\n",
    "\n",
    "# Encode the text\n",
    "encoded_text = huffman_encode(short_text, huffman_codes)\n",
    "compression_ratio = calculate_compression_ratio(short_text, encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357008de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä Compression Results:\")\n",
    "print(f\"   ‚Ä¢ Original size: {len(short_text) * 8} bits ({len(short_text)} chars √ó 8 bits)\")\n",
    "print(f\"   ‚Ä¢ Compressed size: {len(encoded_text)} bits\")\n",
    "print(f\"   ‚Ä¢ Compression ratio: {compression_ratio:.3f}\")\n",
    "print(f\"   ‚Ä¢ Space saved: {(1-compression_ratio)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize code lengths\n",
    "char_frequencies = Counter(short_text)\n",
    "code_lengths = [(char, len(huffman_codes[char]), freq) \n",
    "                for char, freq in char_frequencies.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2bc317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Frequency vs Code Length\n",
    "chars = [c if c != ' ' else 'SP' for c, _, _ in code_lengths[:10]]\n",
    "frequencies = [f for _, _, f in code_lengths[:10]]\n",
    "lengths = [l for _, l, _ in code_lengths[:10]]\n",
    "\n",
    "ax1.scatter(frequencies, lengths, s=100, alpha=0.7)\n",
    "for i, char in enumerate(chars):\n",
    "    ax1.annotate(char, (frequencies[i], lengths[i]), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "ax1.set_xlabel('Character Frequency')\n",
    "ax1.set_ylabel('Huffman Code Length (bits)')\n",
    "ax1.set_title('Frequency vs Code Length')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Code length distribution\n",
    "ax2.bar(range(len(chars)), lengths, alpha=0.7)\n",
    "ax2.set_xlabel('Characters (by frequency)')\n",
    "ax2.set_ylabel('Code Length (bits)')\n",
    "ax2.set_title('Huffman Code Lengths')\n",
    "ax2.set_xticks(range(len(chars)))\n",
    "ax2.set_xticklabels(chars, rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéì Huffman Coding Insights:\")\n",
    "print(\"   ‚Ä¢ Frequent characters get shorter codes\")\n",
    "print(\"   ‚Ä¢ Rare characters get longer codes\")\n",
    "print(\"   ‚Ä¢ This minimizes total message length\")\n",
    "print(\"   ‚Ä¢ Optimal prefix-free coding algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68bea4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary and Connections to Deep Learning\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ LESSON SUMMARY & DEEP LEARNING CONNECTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüîó How These Concepts Connect to Deep Learning:\")\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  GRADIENTS & OPTIMIZATION:\")\n",
    "print(\"   ‚Ä¢ Neural networks use gradients to minimize loss functions\")\n",
    "print(\"   ‚Ä¢ Backpropagation computes gradients for all parameters\")\n",
    "print(\"   ‚Ä¢ Same gradient descent principles, but in high dimensions\")\n",
    "print(\"   ‚Ä¢ Learning rate and momentum prevent getting stuck\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  CROSS ENTROPY & LOSS FUNCTIONS:\")\n",
    "print(\"   ‚Ä¢ Cross entropy is the standard loss for classification\")\n",
    "print(\"   ‚Ä¢ Measures how well predicted probabilities match true labels\")\n",
    "print(\"   ‚Ä¢ Lower cross entropy = better model performance\")\n",
    "print(\"   ‚Ä¢ KL divergence helps compare distributions\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  INFORMATION THEORY & MODEL DESIGN:\")\n",
    "print(\"   ‚Ä¢ Entropy guides feature selection (informative features)\")\n",
    "print(\"   ‚Ä¢ Compression principles inspire model architectures\")\n",
    "print(\"   ‚Ä¢ Information bottleneck theory explains deep learning\")\n",
    "print(\"   ‚Ä¢ Adaptive coding relates to attention mechanisms\")\n",
    "\n",
    "print(\"\\nüìö Next Steps for Further Learning:\")\n",
    "print(\"   ‚Ä¢ Explore multi-dimensional optimization landscapes\")\n",
    "print(\"   ‚Ä¢ Implement backpropagation using gradient principles\")\n",
    "print(\"   ‚Ä¢ Study mutual information in neural networks\")\n",
    "print(\"   ‚Ä¢ Investigate information-theoretic regularization\")\n",
    "\n",
    "print(\"\\nüéâ Great job completing this lesson!\")\n",
    "print(\"You now understand the mathematical foundations that power modern AI!\")\n",
    "\n",
    "# Final interactive element\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üß™ QUICK QUIZ - Test Your Understanding!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "questions = [\n",
    "    {\n",
    "        \"q\": \"What does a positive gradient tell us about a function?\",\n",
    "        \"options\": [\"A) Function is decreasing\", \"B) Function is increasing\", \"C) Function is at minimum\"],\n",
    "        \"answer\": \"B\",\n",
    "        \"explanation\": \"Positive gradient means the function is increasing in that direction.\"\n",
    "    },\n",
    "    {\n",
    "        \"q\": \"When is cross entropy minimized?\",\n",
    "        \"options\": [\"A) When distributions are different\", \"B) When distributions are identical\", \"C) When entropy is maximized\"],\n",
    "        \"answer\": \"B\", \n",
    "        \"explanation\": \"Cross entropy is minimized when predicted and true distributions are identical.\"\n",
    "    },\n",
    "    {\n",
    "        \"q\": \"In Huffman coding, frequent characters get:\",\n",
    "        \"options\": [\"A) Longer codes\", \"B) Shorter codes\", \"C) Random codes\"],\n",
    "        \"answer\": \"B\",\n",
    "        \"explanation\": \"Frequent characters get shorter codes to minimize total message length.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, qa in enumerate(questions, 1):\n",
    "    print(f\"\\n‚ùì Question {i}: {qa['q']}\")\n",
    "    for option in qa['options']:\n",
    "        print(f\"   {option}\")\n",
    "    print(f\"\\n‚úÖ Answer: {qa['answer']} - {qa['explanation']}\")\n",
    "\n",
    "print(\"\\nüåü Congratulations! You've mastered the fundamentals!\")\n",
    "print(\"üöÄ Ready to build some neural networks? Let's go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e4562",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
