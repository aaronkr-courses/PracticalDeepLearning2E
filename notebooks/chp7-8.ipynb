{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac7f3c2",
   "metadata": {},
   "source": [
    "# MNIST NN Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    s = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    e = time.time()-s\n",
    "    loss = clf.loss_\n",
    "    weights = clf.coefs_\n",
    "    biases = clf.intercepts_\n",
    "    params = 0\n",
    "    for w in weights:\n",
    "        params += w.shape[0]*w.shape[1]\n",
    "    for b in biases:\n",
    "        params += b.shape[0]\n",
    "    return [clf.score(x_test, y_test), loss, params, e]\n",
    "\n",
    "def nn(layers, act):\n",
    "    return MLPClassifier(solver=\"sgd\", verbose=False, tol=1e-8,\n",
    "            nesterovs_momentum=False, early_stopping=False,\n",
    "            learning_rate_init=0.001, momentum=0.9, max_iter=200,\n",
    "            hidden_layer_sizes=layers, activation=act)\n",
    "\n",
    "def main():\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    N = 1000\n",
    "    x_train = x_train[:N]\n",
    "    y_train = y_train[:N]\n",
    "    x_test  = x_test[:N]\n",
    "    y_test  = y_test[:N]\n",
    "\n",
    "    layers = [\n",
    "        (1,), (500,), (800,), (1000,), (2000,), (3000,),\n",
    "        (1000,500), (3000,1500),\n",
    "        (2,2,2), (1000,500,250), (2000,1000,500),\n",
    "    ]\n",
    "\n",
    "    for act in [\"relu\", \"logistic\", \"tanh\"]:\n",
    "        print(\"%s:\" % act)\n",
    "        for layer in layers:\n",
    "            scores = []\n",
    "            loss = []\n",
    "            tm = []\n",
    "            for i in range(10):\n",
    "                s,l,params,e = run(x_train, y_train, x_test, y_test, nn(layer,act))\n",
    "                scores.append(s)\n",
    "                loss.append(l)\n",
    "                tm.append(e)\n",
    "            s = np.array(scores)\n",
    "            l = np.array(loss)\n",
    "            t = np.array(tm)\n",
    "            n = np.sqrt(s.shape[0])\n",
    "            print(\"    layers: %14s, score= %0.4f +/- %0.4f, loss = %0.4f +/- %0.4f (params = %6d, time = %0.2f s)\" % \\\n",
    "                (str(layer), s.mean(), s.std()/n, l.mean(), l.std()/n, params, t.mean()))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81795372",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cf850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  file:  mnist_nn_experiments_relu.py\n",
    "#\n",
    "#  Reduced MNIST + NN for Chapter 6.\n",
    "#\n",
    "#  RTK, 13-Oct-2018\n",
    "#  Last update:  30-Dec-2018\n",
    "#\n",
    "###############################################################\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "\n",
    "def nparams(x_train, y_train, clf):\n",
    "    clf.max_iter=1\n",
    "    clf.fit(x_train, y_train)\n",
    "    weights = clf.coefs_\n",
    "    biases = clf.intercepts_\n",
    "    params = 0\n",
    "    for w in weights:\n",
    "        params += w.shape[0]*w.shape[1]\n",
    "    for b in biases:\n",
    "        params += b.shape[0]\n",
    "    return params\n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Train and test\"\"\"\n",
    "\n",
    "    s = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    e = time.time()-s\n",
    "    loss = clf.loss_\n",
    "    weights = clf.coefs_\n",
    "    biases = clf.intercepts_\n",
    "    params = 0\n",
    "    for w in weights:\n",
    "        params += w.shape[0]*w.shape[1]\n",
    "    for b in biases:\n",
    "        params += b.shape[0]\n",
    "    return [clf.score(x_test, y_test), loss, params, e]\n",
    "\n",
    "\n",
    "def nn(layers, act):\n",
    "    \"\"\"Initialize a network\"\"\"\n",
    "\n",
    "    return MLPClassifier(solver=\"sgd\", verbose=False, tol=1e-8,\n",
    "            nesterovs_momentum=False, early_stopping=False,\n",
    "            learning_rate_init=0.001, momentum=0.9, max_iter=200,\n",
    "            hidden_layer_sizes=layers, activation=act)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the experiments for the MNIST data\"\"\"\n",
    "\n",
    "    #  Vector MNIST versions scaled [0,1)\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    #  Reduce the size of the train dataset\n",
    "    N = 20000\n",
    "    x_train = x_train[:N]\n",
    "    y_train = y_train[:N]\n",
    "    x_test  = x_test[:N]\n",
    "    y_test  = y_test[:N]\n",
    "\n",
    "    #  chosen so # params approx same across respective number of layers\n",
    "    layers = [\n",
    "        (1000,), (2000,), (4000,), (8000,),\n",
    "        (700,350), (1150,575), (1850,925), (2850,1425),\n",
    "        (660, 330, 165), (1080,540,270), (1714,857,429), (2620,1310,655),\n",
    "    ]\n",
    "\n",
    "    layers = [(8000,),(2850,1425)]\n",
    "\n",
    "    for layer in layers:\n",
    "        scores = []\n",
    "        loss = []\n",
    "        tm = []\n",
    "        for i in range(5):\n",
    "            s,l,params,e = run(x_train, y_train, x_test, y_test, nn(layer,\"relu\"))\n",
    "            scores.append(s)\n",
    "            loss.append(l)\n",
    "            tm.append(e)\n",
    "        s = np.array(scores)\n",
    "        l = np.array(loss)\n",
    "        t = np.array(tm)\n",
    "        n = np.sqrt(s.shape[0])\n",
    "        print(\"layers: %14s, score= %0.4f +/- %0.4f, loss = %0.4f +/- %0.4f (params = %6d, time = %0.2f s)\" % \\\n",
    "            (str(layer), s.mean(), s.std()/n, l.mean(), l.std()/n, params, t.mean()))\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b95c17",
   "metadata": {},
   "source": [
    "# Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  file:  mnist_nn_experiments_retrain.py\n",
    "#\n",
    "#  Reduced MNIST + NN for Chapter 6.\n",
    "#\n",
    "#  RTK, 15-Oct-2018\n",
    "#  Last update:  15-Oct-2018\n",
    "#\n",
    "###############################################################\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Train and test\"\"\"\n",
    "\n",
    "    s = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    e = time.time()-s\n",
    "    loss = clf.loss_\n",
    "    return [clf.score(x_test, y_test), loss, e]\n",
    "\n",
    "\n",
    "def nn():\n",
    "    \"\"\"Initialize a network\"\"\"\n",
    "\n",
    "    return MLPClassifier(solver=\"sgd\", verbose=False, tol=1e-8,\n",
    "            nesterovs_momentum=False, early_stopping=False,\n",
    "            learning_rate_init=0.001, momentum=0.9, max_iter=50,\n",
    "            hidden_layer_sizes=(1000,500), activation=\"relu\",\n",
    "            batch_size=64)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the experiments for the iris data\"\"\"\n",
    "\n",
    "    #  Vector MNIST versions scaled [0,1)\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    #  training set samples\n",
    "    N = 20000\n",
    "    x = x_train[:N]\n",
    "    y = y_train[:N]\n",
    "    xt= x_test[:N]\n",
    "    yt= y_test[:N]\n",
    "\n",
    "    M = 20\n",
    "    scores = np.zeros(M)\n",
    "    losses = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        s,l,e = run(x, y, xt, yt, nn())\n",
    "        print(\"%03i: score = %0.5f, loss = %0.5f\" % (i,s,l))\n",
    "        scores[i] = s\n",
    "        losses[i] = l\n",
    "\n",
    "    print()\n",
    "    print(\"Scores:  min, max, mean+/-SE: %0.5f, %0.5f, %0.5f +/- %0.5f\" % \\\n",
    "        (scores.min(), scores.max(), scores.mean(), scores.std()/np.sqrt(scores.shape[0])))\n",
    "    print(\"Loss  :  min, max, mean+/-SE: %0.5f, %0.5f, %0.5f +/- %0.5f\" % \\\n",
    "        (losses.min(), losses.max(), losses.mean(), losses.std()/np.sqrt(losses.shape[0])))\n",
    "    print()\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337ba97c",
   "metadata": {},
   "source": [
    "# Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  file:  mnist_nn_experiments_batch_size.py\n",
    "#\n",
    "#  RTK, 14-Oct-2018\n",
    "#  Last update:  07-Jan-2019\n",
    "#\n",
    "###############################################################\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Train and test\"\"\"\n",
    "\n",
    "    s = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    e = time.time()-s\n",
    "    loss = clf.loss_\n",
    "    weights = clf.coefs_\n",
    "    biases = clf.intercepts_\n",
    "    params = 0\n",
    "    for w in weights:\n",
    "        params += w.shape[0]*w.shape[1]\n",
    "    for b in biases:\n",
    "        params += b.shape[0]\n",
    "    return [clf.score(x_test, y_test), loss, params, e, clf.n_iter_]\n",
    "\n",
    "\n",
    "def nn(bz,epochs):\n",
    "    \"\"\"Initialize a network\"\"\"\n",
    "\n",
    "    return MLPClassifier(solver=\"sgd\", verbose=False, tol=1e-8,\n",
    "            nesterovs_momentum=False, early_stopping=False,\n",
    "            learning_rate_init=0.001, momentum=0.9, max_iter=epochs,\n",
    "            hidden_layer_sizes=(1000,500), activation=\"relu\",\n",
    "            batch_size=bz)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the experiments for the iris data\"\"\"\n",
    "\n",
    "    #  Vector MNIST versions scaled [0,1)\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    #  training set samples\n",
    "    N = 16384\n",
    "    x = x_train[:N]\n",
    "    y = y_train[:N]\n",
    "\n",
    "    batch_sizes = [16384,8192,4096,2048,1024,512,256,128,64,32,16,8,4,2]\n",
    "    M = 8192  # set epochs so minibatches is constant\n",
    "\n",
    "    for bz in batch_sizes:\n",
    "        print(\"batch size = %4d:\" % bz)\n",
    "        #epochs = 100 \n",
    "        epochs = (M*bz) // N\n",
    "        if (epochs < 1):\n",
    "            epochs = 1\n",
    "        scores = []\n",
    "        loss = []\n",
    "        tm = []\n",
    "        for i in range(5):\n",
    "            s,l,p,e,m = run(x, y, x_test, y_test, nn(bz,epochs))\n",
    "            scores.append(s)\n",
    "            loss.append(l)\n",
    "            tm.append(e)\n",
    "            print(\"    score = %0.5f, loss = %0.5f, epochs = %d, actual = %d\" % (s,l,epochs,m))\n",
    "        scores = np.array(scores)\n",
    "        loss = np.array(loss)\n",
    "        sm = scores.mean()\n",
    "        se = scores.std() / np.sqrt(scores.shape[0])\n",
    "        lm = loss.mean()\n",
    "        le = loss.std() / np.sqrt(loss.shape[0])\n",
    "        print(\"    final score = %0.5f +/- %0.5f, loss = %0.5f +/- %0.5f, epochs = %d\" % (sm,se,lm,le,epochs))\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3256d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def main():\n",
    "    # epochs == 100\n",
    "    bz = np.array([2,4,8,16,32,64,128,256,512,1024,2048,4096,8192,16384])\n",
    "    sc = np.array([0.97174,0.96976,0.96876,0.96734,0.96596,0.96332,0.95684,0.94564,0.93312,0.91788,0.90046,0.87318,0.82926,0.76104])\n",
    "    ec = np.array([0.00006,0.00034,0.00007,0.00012,0.00011,0.00040,0.00042,0.00040,0.00038,0.00040,0.00078,0.00086,0.00203,0.00591])\n",
    "\n",
    "    # minibatches = 8192\n",
    "    sc0= np.array([0.93658,0.94556,0.94856,0.94916,0.95012,0.94946,0.95068,0.95038,0.95112,0.95030,0.95066,0.95028,0.94992,0.94994])\n",
    "    ec0= np.array([0.00214,0.00078,0.00070,0.00115,0.00025,0.00028,0.00053,0.00041,0.00045,0.00023,0.00032,0.00058,0.00044,0.00022])\n",
    "\n",
    "    plt.errorbar(bz,sc,ec,marker='o',color='k', fillstyle='none')\n",
    "    plt.errorbar(bz,sc0,ec0,marker='s',color='k', fillstyle='none')\n",
    "    plt.xlabel(\"Minibatch Size\")\n",
    "    plt.ylabel(\"Mean Score\")\n",
    "    plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "    plt.savefig(\"mnist_nn_experiments_batch_size_plot.png\", format=\"png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38d203",
   "metadata": {},
   "source": [
    "# Base Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  file:  mnist_nn_experiments_base_lr.py\n",
    "#\n",
    "#  Reduced MNIST + NN for Chapter 6.\n",
    "#\n",
    "#  RTK, 15-Oct-2018\n",
    "#  Last update:  15-Oct-2018\n",
    "#\n",
    "###############################################################\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Train and test\"\"\"\n",
    "\n",
    "    s = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    e = time.time()-s\n",
    "    loss = clf.loss_\n",
    "    return [clf.score(x_test, y_test), loss, e]\n",
    "\n",
    "\n",
    "def nn(base_lr, epochs):\n",
    "    \"\"\"Initialize a network\"\"\"\n",
    "\n",
    "    return MLPClassifier(solver=\"sgd\", verbose=False, tol=1e-8,\n",
    "            nesterovs_momentum=False, early_stopping=False,\n",
    "            learning_rate_init=base_lr, momentum=0.9, max_iter=epochs,\n",
    "            hidden_layer_sizes=(1000,500), activation=\"relu\",\n",
    "            learning_rate=\"constant\", batch_size=64)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the experiments for the MNIST vector data\"\"\"\n",
    "\n",
    "    #  Vector MNIST versions scaled [0,1)\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    #  training set samples\n",
    "    N = 20000\n",
    "    x = x_train[:N]\n",
    "    y = y_train[:N]\n",
    "    xt= x_test[:N]\n",
    "    yt= y_test[:N]\n",
    "\n",
    "    base_lr = [0.2,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]\n",
    "\n",
    "    for lr in base_lr:\n",
    "        s,l,e = run(x, y, xt, yt, nn(lr,50))\n",
    "        print(\"base_lr = %0.5f, score = %0.5f, loss = %0.5f, epochs = %d\" % (lr,s,l,50))\n",
    "    print()\n",
    "\n",
    "    #  choose epochs so base_lr * epochs == 1.5\n",
    "    epochs = [8, 15, 30, 150, 300, 1500, 3000, 15000]\n",
    "\n",
    "    for i in range(len(base_lr)):\n",
    "        s,l,e = run(x, y, xt, yt, nn(base_lr[i], epochs[i]))\n",
    "        print(\"base_lr = %0.5f, score = %0.5f, loss = %0.5f, epochs = %d, time = %0.3f\" % (base_lr[i],s,l,epochs[i],e))\n",
    "    print()\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b173e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def main():\n",
    "    # score by base_lr, fixed epochs\n",
    "    sc0 = np.array([0.91870,0.95070,0.96050,0.97120,0.97260,0.97540,0.97630,0.94800])\n",
    "    lr = np.array([0.00010,0.00050,0.00100,0.00500,0.01000,0.05000,0.10000,0.20000])\n",
    "\n",
    "    # score by base_lr, lr * epochs = 1.5\n",
    "    sc1 = np.array([0.96990,0.97030,0.97060,0.97240,0.97310,0.97590,0.97340,0.95550])\n",
    "\n",
    "    plt.semilogx(lr,sc0,marker='o',color='k', fillstyle='none')\n",
    "    plt.semilogx(lr,sc1,marker='s',color='k', fillstyle='none')\n",
    "    plt.xlabel(\"Learning rate ($\\eta$)\")\n",
    "    plt.ylabel(\"Test Score\")\n",
    "    plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "    plt.savefig(\"mnist_nn_experiments_base_lr_plot.png\", format=\"png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "#base_lr = 0.20000, score = 0.94800, loss = 0.09340, epochs = 50\n",
    "#base_lr = 0.10000, score = 0.97630, loss = 0.00207, epochs = 50\n",
    "#base_lr = 0.05000, score = 0.97540, loss = 0.00162, epochs = 50\n",
    "#base_lr = 0.01000, score = 0.97260, loss = 0.00229, epochs = 50\n",
    "#base_lr = 0.00500, score = 0.97120, loss = 0.00413, epochs = 50\n",
    "#base_lr = 0.00100, score = 0.96050, loss = 0.06542, epochs = 50\n",
    "#base_lr = 0.00050, score = 0.95070, loss = 0.13361, epochs = 50\n",
    "#base_lr = 0.00010, score = 0.91870, loss = 0.29111, epochs = 50\n",
    "#\n",
    "#base_lr = 0.20000, score = 0.95550, loss = 0.07414, epochs = 8, time = 71.445\n",
    "#base_lr = 0.10000, score = 0.97340, loss = 0.00946, epochs = 15, time = 132.122\n",
    "#base_lr = 0.05000, score = 0.97590, loss = 0.00168, epochs = 30, time = 268.132\n",
    "#base_lr = 0.01000, score = 0.97310, loss = 0.00163, epochs = 150, time = 1385.001\n",
    "#base_lr = 0.00500, score = 0.97240, loss = 0.00163, epochs = 300, time = 2741.335\n",
    "#base_lr = 0.00100, score = 0.97060, loss = 0.00163, epochs = 1500, time = 13232.182\n",
    "#base_lr = 0.00050, score = 0.97030, loss = 0.00163, epochs = 3000, time = 26478.357\n",
    "#base_lr = 0.00010, score = 0.96990, loss = 0.00162, epochs = 15000, time = 135642.231\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e129241",
   "metadata": {},
   "source": [
    "# Training-Set Size (Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efcfe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  file:  mnist_nn_experiments_samples.py\n",
    "#\n",
    "#  Reduced MNIST + NN for Chapter 6.\n",
    "#\n",
    "#  RTK, 15-Oct-2018\n",
    "#  Last update:  15-Oct-2018\n",
    "#\n",
    "###############################################################\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Train and test\"\"\"\n",
    "\n",
    "    s = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    e = time.time()-s\n",
    "    loss = clf.loss_\n",
    "    return [clf.score(x_test, y_test), loss, e]\n",
    "\n",
    "\n",
    "def nn(epochs):\n",
    "    \"\"\"Initialize a network\"\"\"\n",
    "\n",
    "    return MLPClassifier(solver=\"sgd\", verbose=False, tol=1e-8,\n",
    "            nesterovs_momentum=False, early_stopping=False,\n",
    "            learning_rate_init=0.05, momentum=0.9, max_iter=epochs,\n",
    "            hidden_layer_sizes=(1000,500), activation=\"relu\",\n",
    "            learning_rate=\"constant\", batch_size=100)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the experiments for the MNIST vector data\"\"\"\n",
    "\n",
    "    #  Vector MNIST versions scaled [0,1)\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    #  training set samples\n",
    "    N = [100, 200, 300, 400, 500, 600,\n",
    "        700, 800, 900, 1000, 1500, 2000, 2500, 3000, 3500, 4000,\n",
    "        4500, 5000, 7500, 10000, 15000, 20000, 25000, 30000]\n",
    "    M = 5\n",
    "\n",
    "    for n in N:\n",
    "        scores = np.zeros(M)\n",
    "        print(\"samples = %5d\" % n)\n",
    "        for i in range(M):\n",
    "            idx = np.argsort(np.random.random(y_train.shape[0]))\n",
    "            x_train = x_train[idx]\n",
    "            y_train = y_train[idx]\n",
    "            x = x_train[:n]\n",
    "            y = y_train[:n]\n",
    "            epochs = int((100.0/n)*1000) # epochs to take 1,000 SGD steps\n",
    "            s,l,e = run(x, y, x_test, y_test, nn(epochs))\n",
    "            scores[i] = s\n",
    "            print(\"    score = %0.5f, loss = %0.5f, epochs = %d, training time = %0.3f\" % (s,l,epochs,e))\n",
    "        print(\"    mean score = %0.5f +/- %0.5f\" % (scores.mean(), scores.std()/np.sqrt(M)))\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3105a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "def main():\n",
    "    x = [100, 200, 300, 400, 500, 600,700, 800, 900, 1000, \n",
    "        1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 7500, \n",
    "        10000, 15000, 20000, 25000, 30000]\n",
    "    y = [0.75330, 0.81608, 0.85652, 0.86402, 0.87372, 0.87764,\n",
    "        0.88382, 0.89334, 0.89644, 0.90168, 0.91856, 0.92490,\n",
    "        0.92966, 0.93690, 0.94190, 0.94424, 0.94964, 0.95032,\n",
    "        0.95862, 0.96526, 0.96664, 0.96944, 0.96980, 0.96926]\n",
    "  \n",
    "    plt.plot(x,y, marker=\"o\", color=\"k\", fillstyle='none')\n",
    "    plt.xlabel(\"Number of training samples\")\n",
    "    plt.ylabel(\"Mean test score\")\n",
    "    plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "    plt.savefig(\"mnist_nn_experiments_samples_plot.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20255137",
   "metadata": {},
   "source": [
    "# L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686dc91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  file:  mnist_nn_experiments_L2.py\n",
    "#\n",
    "#  RTK, 19-Oct-2018\n",
    "#  Last update:  20-Oct-2018\n",
    "#\n",
    "###############################################################\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def epoch(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Results for a single epoch\"\"\"\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    train_loss = clf.loss_\n",
    "    train_err = 1.0 - clf.score(x_train, y_train)\n",
    "    val_err = 1.0 - clf.score(x_test, y_test)\n",
    "    clf.warm_start = True\n",
    "    return [train_loss, train_err, val_err]\n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf, max_iter):\n",
    "    \"\"\"Train and test\"\"\"\n",
    "\n",
    "    train_loss = []\n",
    "    train_err = []\n",
    "    val_err = []\n",
    "\n",
    "    clf.max_iter = 1  # one epoch at a time\n",
    "    for i in range(max_iter):\n",
    "        tl, terr, verr = epoch(x_train, y_train, x_test, y_test, clf)\n",
    "        train_loss.append(tl)\n",
    "        train_err.append(terr)\n",
    "        val_err.append(verr)\n",
    "        print(\"    %4d: val_err = %0.5f\" % (i, val_err[-1]))\n",
    "\n",
    "    wavg = 0.0\n",
    "    n = 0\n",
    "    for w in clf.coefs_:\n",
    "        wavg += w.sum()\n",
    "        n += w.size\n",
    "    wavg /= n\n",
    "\n",
    "    return [train_loss, train_err, val_err, wavg]\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Plot the training and validation losses.\"\"\"\n",
    "\n",
    "    os.system(\"rm -rf mnist_nn_experiments_L2\")\n",
    "    os.system(\"mkdir mnist_nn_experiments_L2\")\n",
    "\n",
    "    #  Vector MNIST versions scaled [0,1)\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    #  Reduce the size of the train dataset\n",
    "    x_train = x_train[:3000]\n",
    "    y_train = y_train[:3000]\n",
    "\n",
    "    #  L2 values\n",
    "    colors= ['k','r','b','g','c']\n",
    "    alpha = [0.0,0.1,0.2,0.3,0.4]\n",
    "    epochs = 10000 \n",
    "\n",
    "    for k,a in enumerate(alpha):\n",
    "        nn = MLPClassifier(solver=\"sgd\", verbose=False, tol=0,\n",
    "                nesterovs_momentum=False,\n",
    "                early_stopping=False,\n",
    "                learning_rate_init=0.01,\n",
    "                momentum=0.0,\n",
    "                hidden_layer_sizes=(100,50),\n",
    "                activation=\"relu\",\n",
    "                alpha=a,\n",
    "                learning_rate=\"constant\",\n",
    "                batch_size=64,\n",
    "                max_iter=1)\n",
    "        tt = \"alpha = %0.6f\" % a\n",
    "        print(tt)\n",
    "        train_loss, train_err, val_err, wavg = run(x_train, y_train, x_test, y_test, nn, epochs)\n",
    "        print(\"    final: train error: %0.5f, val error: %0.5f, mean weight value = %0.8f\"  % \\\n",
    "            (train_err[-1], val_err[-1], wavg))\n",
    "        print()\n",
    "        if (k==0):\n",
    "            plt.plot(val_err, color=colors[k], linewidth=3)\n",
    "        else:\n",
    "            plt.plot(val_err, color=colors[k])\n",
    "        np.save(\"mnist_nn_experiments_L2/train_error_%0.6f.npy\" % a, train_err)\n",
    "        np.save(\"mnist_nn_experiments_L2/train_loss_%0.6f.npy\" % a, train_loss)\n",
    "        np.save(\"mnist_nn_experiments_L2/val_error_%0.6f.npy\" % a, val_err)\n",
    "        np.save(\"mnist_nn_experiments_L2/mean_weight_%0.6f.npy\" % a, np.array(wavg))\n",
    "    plt.ylim((0.03,0.17))\n",
    "    plt.xlabel(\"Epochs\", fontsize=16)\n",
    "    plt.ylabel(\"Error\", fontsize=16)\n",
    "    plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "    pname = \"mnist_nn_experiments_L2/mnist_nn_experiments_L2_plot.png\"\n",
    "    plt.savefig(pname, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94570fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def main():\n",
    "    d0 = np.load(\"mnist_nn_experiments_L2/val_error_0.000000.npy\")\n",
    "    d1 = np.load(\"mnist_nn_experiments_L2/val_error_0.100000.npy\")\n",
    "    d2 = np.load(\"mnist_nn_experiments_L2/val_error_0.200000.npy\")\n",
    "    d3 = np.load(\"mnist_nn_experiments_L2/val_error_0.300000.npy\")\n",
    "    d4 = np.load(\"mnist_nn_experiments_L2/val_error_0.400000.npy\")\n",
    "\n",
    "    plt.plot(d0, color=\"k\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(d1, color=\"r\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(d2, color=\"g\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(d3, color=\"b\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(d4, color=\"c\", linewidth=1, linestyle=\"-\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Validation Error\")\n",
    "    plt.ylim((0.05,0.1))\n",
    "    plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "    plt.savefig(\"mnist_nn_experiments_L2_plot.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df2263",
   "metadata": {},
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74318e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  file:  mnist_nn_experiments_momentum.py\n",
    "#\n",
    "#  RTK, 19-Oct-2018\n",
    "#  Last update:  03-Feb-2019\n",
    "#\n",
    "###############################################################\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def epoch(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Results for a single epoch\"\"\"\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    train_loss = clf.loss_\n",
    "    train_err = 1.0 - clf.score(x_train, y_train)\n",
    "    val_err = 1.0 - clf.score(x_test, y_test)\n",
    "    clf.warm_start = True\n",
    "    return [train_loss, train_err, val_err]\n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf, max_iter):\n",
    "    \"\"\"Train and test\"\"\"\n",
    "\n",
    "    train_loss = []\n",
    "    train_err = []\n",
    "    val_err = []\n",
    "\n",
    "    clf.max_iter = 1  # one epoch at a time\n",
    "    for i in range(max_iter):\n",
    "        tl, terr, verr = epoch(x_train, y_train, x_test, y_test, clf)\n",
    "        train_loss.append(tl)\n",
    "        train_err.append(terr)\n",
    "        val_err.append(verr)\n",
    "        print(\"    %4d: val_err = %0.5f\" % (i, val_err[-1]))\n",
    "\n",
    "    return [train_loss, train_err, val_err]\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Plot the training and validation losses.\"\"\"\n",
    "\n",
    "    os.system(\"rm -rf mnist_nn_experiments_momentum\")\n",
    "    os.system(\"mkdir mnist_nn_experiments_momentum\")\n",
    "\n",
    "    #  Vector MNIST versions scaled [0,1)\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    #  Reduce the size of the train dataset\n",
    "    x_train = x_train[:3000]\n",
    "    y_train = y_train[:3000]\n",
    "\n",
    "    #  momentum values\n",
    "    colors= ['k','r','b','g','c','m']\n",
    "    momentum = [0.0,0.3,0.5,0.7,0.9,0.99]\n",
    "    epochs = 10000 \n",
    "\n",
    "    for k,m in enumerate(momentum):\n",
    "        nn = MLPClassifier(solver=\"sgd\", verbose=False, tol=0,\n",
    "                nesterovs_momentum=False,\n",
    "                early_stopping=False,\n",
    "                learning_rate_init=0.01,\n",
    "                momentum=m,\n",
    "                hidden_layer_sizes=(100,50),\n",
    "                activation=\"relu\",\n",
    "                alpha=0.0001,\n",
    "                learning_rate=\"constant\",\n",
    "                batch_size=64,\n",
    "                max_iter=1)\n",
    "        print(\"momentum = %0.1f\" % m)\n",
    "        train_loss, train_err, val_err = run(x_train, y_train, x_test, y_test, nn, epochs)\n",
    "        print(\"    final: train error: %0.5f, val error: %0.5f\"  % \\\n",
    "            (train_err[-1], val_err[-1]))\n",
    "        print()\n",
    "        if (k==0):\n",
    "            plt.plot(val_err, color=colors[k], linewidth=3)\n",
    "        else:\n",
    "            plt.plot(val_err, color=colors[k])\n",
    "        np.save(\"mnist_nn_experiments_momentum/train_error_%0.2f.npy\" % m, train_err)\n",
    "        np.save(\"mnist_nn_experiments_momentum/train_loss_%0.2f.npy\" % m, train_loss)\n",
    "        np.save(\"mnist_nn_experiments_momentum/val_error_%0.2f.npy\" % m, val_err)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.tight_layout()\n",
    "    pname = \"mnist_nn_experiments_momentum/mnist_nn_experiments_momentum_plot.png\"\n",
    "    plt.savefig(pname, format=\"png\", dpi=600)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def main():\n",
    "    d0 = np.load(\"mnist_nn_experiments_momentum/val_error_0.00.npy\")\n",
    "    d1 = np.load(\"mnist_nn_experiments_momentum/val_error_0.30.npy\")\n",
    "    d2 = np.load(\"mnist_nn_experiments_momentum/val_error_0.50.npy\")\n",
    "    d3 = np.load(\"mnist_nn_experiments_momentum/val_error_0.70.npy\")\n",
    "    d4 = np.load(\"mnist_nn_experiments_momentum/val_error_0.90.npy\")\n",
    "    d5 = np.load(\"mnist_nn_experiments_momentum/val_error_0.99.npy\")\n",
    "\n",
    "    plt.plot(d0, color=\"k\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(d1, color=\"r\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(d2, color=\"g\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(d3, color=\"b\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(d4, color=\"c\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(d5, color=\"m\", linewidth=1, linestyle=\"-\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Test Error\")\n",
    "    plt.ylim((0.05,0.1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"mnist_nn_experiments_momentum_plot.pdf\", type=\"pdf\", dpi=600)\n",
    "    plt.savefig(\"mnist_nn_experiments_momentum_plot.png\", type=\"png\", dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408fe780",
   "metadata": {},
   "source": [
    "## 30k Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ca605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  file:  mnist_nn_experiments_30k_momentum.py\n",
    "#\n",
    "#  RTK, 19-Oct-2018\n",
    "#  Last update:  17-Dec-2019\n",
    "#\n",
    "###############################################################\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def epoch(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Results for a single epoch\"\"\"\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    train_loss = clf.loss_\n",
    "    train_err = 1.0 - clf.score(x_train, y_train)\n",
    "    val_err = 1.0 - clf.score(x_test, y_test)\n",
    "    clf.warm_start = True\n",
    "    return [train_loss, train_err, val_err]\n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf, max_iter):\n",
    "    \"\"\"Train and test\"\"\"\n",
    "\n",
    "    train_loss = []\n",
    "    train_err = []\n",
    "    val_err = []\n",
    "\n",
    "    clf.max_iter = 1  # one epoch at a time\n",
    "    for i in range(max_iter):\n",
    "        tl, terr, verr = epoch(x_train, y_train, x_test, y_test, clf)\n",
    "        train_loss.append(tl)\n",
    "        train_err.append(terr)\n",
    "        val_err.append(verr)\n",
    "        print(\"    %4d: val_err = %0.5f\" % (i, val_err[-1]))\n",
    "\n",
    "    return [train_loss, train_err, val_err]\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Plot the training and validation losses.\"\"\"\n",
    "\n",
    "    os.system(\"rm -rf mnist_nn_experiments_momentum\")\n",
    "    os.system(\"mkdir mnist_nn_experiments_momentum\")\n",
    "\n",
    "    #  Vector MNIST versions scaled [0,1)\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    #  Reduce the size of the train dataset\n",
    "    x_train = x_train[:30000]\n",
    "    y_train = y_train[:30000]\n",
    "\n",
    "    #  momentum values\n",
    "    colors= ['k','r','b','g','c','m']\n",
    "    momentum = [0.0,0.3,0.5,0.7,0.9,0.99]\n",
    "    epochs = 100\n",
    "\n",
    "    for k,m in enumerate(momentum):\n",
    "        nn = MLPClassifier(solver=\"sgd\", verbose=False, tol=0,\n",
    "                nesterovs_momentum=False,\n",
    "                early_stopping=False,\n",
    "                learning_rate_init=0.01,\n",
    "                momentum=m,\n",
    "                hidden_layer_sizes=(100,50),\n",
    "                activation=\"relu\",\n",
    "                alpha=0.0001,\n",
    "                learning_rate=\"constant\",\n",
    "                batch_size=64,\n",
    "                max_iter=1)\n",
    "        print(\"momentum = %0.1f\" % m)\n",
    "        train_loss, train_err, val_err = run(x_train, y_train, x_test, y_test, nn, epochs)\n",
    "        print(\"    final: train error: %0.5f, val error: %0.5f\"  % \\\n",
    "            (train_err[-1], val_err[-1]))\n",
    "        print()\n",
    "        if (k==0):\n",
    "            plt.plot(val_err, color=colors[k], linewidth=3)\n",
    "        else:\n",
    "            plt.plot(val_err, color=colors[k])\n",
    "        np.save(\"mnist_nn_experiments_momentum/train_error_30k_%0.2f.npy\" % m, train_err)\n",
    "        np.save(\"mnist_nn_experiments_momentum/train_loss_30k_%0.2f.npy\" % m, train_loss)\n",
    "        np.save(\"mnist_nn_experiments_momentum/val_error_30k_%0.2f.npy\" % m, val_err)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.tight_layout()\n",
    "    pname = \"mnist_nn_experiments_momentum/mnist_nn_experiments_30k_momentum_plot.png\"\n",
    "    plt.savefig(pname, format=\"png\", dpi=600)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858848c",
   "metadata": {},
   "source": [
    "# Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bc4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  file:  mnist_nn_experiments_init.py\n",
    "#\n",
    "#  RTK, 20-Oct-2018\n",
    "#  Last update:  27-Nov-2022\n",
    "#\n",
    "###############################################################\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "\n",
    "#\n",
    "#  Possible weight init methods\n",
    "#\n",
    "class Classifier(MLPClassifier):\n",
    "    \"\"\"Subclass MLPClassifier to use custom weight initialization\"\"\"\n",
    "\n",
    "    def _init_coef(self, fan_in, fan_out, dtype):\n",
    "        \"\"\"Custom weight initialization\"\"\"\n",
    "\n",
    "        if (self.init_scheme == 0):\n",
    "            #  Glorot initialization\n",
    "            weights, biases = super(Classifier, self)._init_coef(fan_in, fan_out, dtype)\n",
    "        elif (self.init_scheme == 1):\n",
    "            #  small uniformly distributed weights\n",
    "            weights = 0.01*(np.random.random((fan_in, fan_out))-0.5)\n",
    "            biases = np.zeros(fan_out)\n",
    "        elif (self.init_scheme == 2):\n",
    "            #  small Gaussian weights\n",
    "            weights = 0.005*(np.random.normal(size=(fan_in, fan_out)))\n",
    "            biases = np.zeros(fan_out)\n",
    "        elif (self.init_scheme == 3):\n",
    "            #  He initialization for relu\n",
    "            weights = np.random.normal(size=(fan_in, fan_out))*  \\\n",
    "                        np.sqrt(2.0/fan_in)\n",
    "            biases = np.zeros(fan_out)\n",
    "        elif (self.init_scheme == 4):\n",
    "            #  Alternate Xavier\n",
    "            weights = np.random.normal(size=(fan_in, fan_out))*  \\\n",
    "                        np.sqrt(1.0/fan_in)\n",
    "            biases = np.zeros(fan_out)\n",
    "\n",
    "        return weights.astype(dtype, copy=False), biases.astype(dtype, copy=False)\n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf, epochs):\n",
    "    \"\"\"Train and test\"\"\"\n",
    "\n",
    "    test_err = []\n",
    "    clf.max_iter = 1\n",
    "    for i in range(epochs):\n",
    "        clf.fit(x_train, y_train)\n",
    "        terr = 1.0 - clf.score(x_test, y_test)\n",
    "        print(\"    test error %0.5f\" % terr)\n",
    "        clf.warm_start = True\n",
    "        test_err.append(terr)\n",
    "    return test_err\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Plot the training and validation losses.\"\"\"\n",
    "\n",
    "    outdir = \"mnist_nn_experiments_init\"\n",
    "    os.system(\"rm -rf %s\" % outdir)\n",
    "    os.system(\"mkdir %s\" % outdir)\n",
    "\n",
    "    #  Vector MNIST versions scaled [0,1)\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    #  Reduce the size of the train dataset\n",
    "    x_train = x_train[:6000]\n",
    "    y_train = y_train[:6000]\n",
    "    epochs = 4000 \n",
    "    init_types = 5\n",
    "    trainings = 10\n",
    "\n",
    "    test_err = np.zeros((trainings, init_types, epochs)) \n",
    "\n",
    "    for i in range(trainings):\n",
    "        for k in range(init_types):\n",
    "            print(\"Training %d, init scheme %d:\" % (i,k))\n",
    "            nn = Classifier(solver=\"sgd\", verbose=False, tol=0,\n",
    "                   nesterovs_momentum=False, early_stopping=False, learning_rate_init=0.01,\n",
    "                   momentum=0.9, hidden_layer_sizes=(100,50), activation=\"relu\",\n",
    "                   alpha=0.2, learning_rate=\"constant\", batch_size=64, max_iter=1)\n",
    "            nn.init_scheme = k\n",
    "            test_err[i,k,:] = run(x_train, y_train, x_test, y_test, nn, epochs)\n",
    "\n",
    "    np.save(\"mnist_nn_experiments_init_results.npy\", test_err)\n",
    "\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddda5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n",
    "\n",
    "\n",
    "def main():\n",
    "    d = np.load(\"mnist_nn_experiments_init_results.npy\")\n",
    "    d0 = d[:,0,:].mean(axis=0)\n",
    "    d1 = d[:,1,:].mean(axis=0)\n",
    "    d2 = d[:,2,:].mean(axis=0)\n",
    "    d3 = d[:,3,:].mean(axis=0)\n",
    "    d4 = d[:,4,:].mean(axis=0)\n",
    "\n",
    "    plt.plot(smooth(d0,53,\"flat\"), color=\"k\", linewidth=2, linestyle=\"-\")\n",
    "    plt.plot(smooth(d1,53,\"flat\"), color=\"r\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(smooth(d2,53,\"flat\"), color=\"g\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(smooth(d3,53,\"flat\"), color=\"b\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(smooth(d4,53,\"flat\"), color=\"c\", linewidth=1, linestyle=\"-\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Test Error\")\n",
    "    plt.ylim((0.04,0.055))\n",
    "    plt.xlim((75,4000))\n",
    "    plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "    plt.savefig(\"mnist_nn_experiments_init_plot.png\", dpi=300)\n",
    "    plt.savefig(\"mnist_nn_experiments_init_plot.eps\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b9cda",
   "metadata": {},
   "source": [
    "# Feature Ordering (Scrambled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eccbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  file:  mnist_nn_experiments_scrambled.py\n",
    "#\n",
    "#  Reduced MNIST + NN for Chapter 6.\n",
    "#\n",
    "#  RTK, 22-Oct-2018\n",
    "#  Last update:  22-Oct-2018\n",
    "#\n",
    "###############################################################\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "\n",
    "def epoch(x_train, y_train, x_test, y_test, clf):\n",
    "    \"\"\"Results for a single epoch\"\"\"\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    train_loss = clf.loss_\n",
    "    train_err = 1.0 - clf.score(x_train, y_train)\n",
    "    val_err = 1.0 - clf.score(x_test, y_test)\n",
    "    clf.warm_start = True\n",
    "    return [train_loss, train_err, val_err]\n",
    "\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf, max_iter):\n",
    "    \"\"\"Train and test\"\"\"\n",
    "\n",
    "    train_loss = []\n",
    "    train_err = []\n",
    "    val_err = []\n",
    "\n",
    "    clf.max_iter = 1  # one epoch at a time\n",
    "    for i in range(max_iter):\n",
    "        tl, terr, verr = epoch(x_train, y_train, x_test, y_test, clf)\n",
    "        train_loss.append(tl)\n",
    "        train_err.append(terr)\n",
    "        val_err.append(verr)\n",
    "        print \"    %4d: val_err = %0.5f\" % (i, val_err[-1])\n",
    "\n",
    "    wavg = 0.0\n",
    "    n = 0\n",
    "    for w in clf.coefs_:\n",
    "        wavg += w.sum()\n",
    "        n += w.size\n",
    "    wavg /= n\n",
    "\n",
    "    return [train_loss, train_err, val_err, wavg]\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Plot the training and validation losses.\"\"\"\n",
    "\n",
    "    outdir = \"mnist_nn_experiments_scrambled\"\n",
    "    os.system(\"rm -rf %s\" % outdir)\n",
    "    os.system(\"mkdir %s\" % outdir)\n",
    "\n",
    "    #  Vector MNIST versions scaled [0,1)\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")/256.0\n",
    "    xstrain = np.load(\"../data/mnist/mnist_train_scrambled_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")/256.0\n",
    "    xstest = np.load(\"../data/mnist/mnist_test_scrambled_vectors.npy\").astype(\"float64\")/256.0\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    #  Reduce the size of the train dataset\n",
    "    x_train = x_train[:6000]\n",
    "    y_train = y_train[:6000]\n",
    "    xstrain = xstrain[:6000]\n",
    "    epochs = 6000 \n",
    "\n",
    "    print \"Unscrambled\"\n",
    "    nn = MLPClassifier(solver=\"sgd\", verbose=False, tol=0,\n",
    "           nesterovs_momentum=False, early_stopping=False, learning_rate_init=0.01,\n",
    "           momentum=0.9, hidden_layer_sizes=(100,50), activation=\"relu\",\n",
    "           alpha=0.2, learning_rate=\"constant\", batch_size=64, max_iter=1)\n",
    "    train_loss, train_err, val_err, wavg = run(x_train, y_train, x_test, y_test, nn, epochs)\n",
    "    print \"    final: train error: %0.5f, val error: %0.5f, mean weight value = %0.8f\"  % \\\n",
    "        (train_err[-1], val_err[-1], wavg)\n",
    "    print\n",
    "    np.save(outdir + (\"/train_error.npy\"), train_err)\n",
    "    np.save(outdir + (\"/train_loss.npy\"), train_loss)\n",
    "    np.save(outdir + (\"/val_error.npy\"), val_err)\n",
    "    np.save(outdir + (\"/mean_weight.npy\"), np.array(wavg))\n",
    "\n",
    "    print \"Scrambled\"\n",
    "    nn = MLPClassifier(solver=\"sgd\", verbose=False, tol=0,\n",
    "           nesterovs_momentum=False, early_stopping=False, learning_rate_init=0.01,\n",
    "           momentum=0.9, hidden_layer_sizes=(100,50), activation=\"relu\",\n",
    "           alpha=0.2, learning_rate=\"constant\", batch_size=64, max_iter=1)\n",
    "    train_loss, train_err, val_err, wavg = run(xstrain, y_train, xstest, y_test, nn, epochs)\n",
    "    print \"    final: train error: %0.5f, val error: %0.5f, mean weight value = %0.8f\"  % \\\n",
    "        (train_err[-1], val_err[-1], wavg)\n",
    "    print\n",
    "    np.save(outdir + (\"/train_error_scrambled.npy\"), train_err)\n",
    "    np.save(outdir + (\"/train_loss_scrambled.npy\"), train_loss)\n",
    "    np.save(outdir + (\"/val_error_scrambled.npy\"), val_err)\n",
    "    np.save(outdir + (\"/mean_weight_scrambled.npy\"), np.array(wavg))\n",
    "\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError, \"smooth only accepts 1 dimension arrays.\"\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError, \"Input vector needs to be bigger than window size.\"\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError, \"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\"\n",
    "\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n",
    "\n",
    "\n",
    "def main():\n",
    "    c0 = np.load(\"mnist_nn_experiments_scrambled_run0/val_error.npy\")\n",
    "    c1 = np.load(\"mnist_nn_experiments_scrambled_run1/val_error.npy\")\n",
    "    c2 = np.load(\"mnist_nn_experiments_scrambled_run2/val_error.npy\")\n",
    "    c3 = np.load(\"mnist_nn_experiments_scrambled_run3/val_error.npy\")\n",
    "    c4 = np.load(\"mnist_nn_experiments_scrambled_run4/val_error.npy\")\n",
    "    c5 = np.load(\"mnist_nn_experiments_scrambled_run5/val_error.npy\")\n",
    "    c6 = np.load(\"mnist_nn_experiments_scrambled_run6/val_error.npy\")\n",
    "    c7 = np.load(\"mnist_nn_experiments_scrambled_run7/val_error.npy\")\n",
    "    c8 = np.load(\"mnist_nn_experiments_scrambled_run8/val_error.npy\")\n",
    "    c9 = np.load(\"mnist_nn_experiments_scrambled_run9/val_error.npy\")\n",
    "    c = np.array([c0,c1,c2,c3,c4,c5,c6,c7,c8,c9])\n",
    "\n",
    "    d0 = np.load(\"mnist_nn_experiments_scrambled_run0/val_error_scrambled.npy\")\n",
    "    d1 = np.load(\"mnist_nn_experiments_scrambled_run1/val_error_scrambled.npy\")\n",
    "    d2 = np.load(\"mnist_nn_experiments_scrambled_run2/val_error_scrambled.npy\")\n",
    "    d3 = np.load(\"mnist_nn_experiments_scrambled_run3/val_error_scrambled.npy\")\n",
    "    d4 = np.load(\"mnist_nn_experiments_scrambled_run4/val_error_scrambled.npy\")\n",
    "    d5 = np.load(\"mnist_nn_experiments_scrambled_run5/val_error_scrambled.npy\")\n",
    "    d6 = np.load(\"mnist_nn_experiments_scrambled_run6/val_error_scrambled.npy\")\n",
    "    d7 = np.load(\"mnist_nn_experiments_scrambled_run7/val_error_scrambled.npy\")\n",
    "    d8 = np.load(\"mnist_nn_experiments_scrambled_run8/val_error_scrambled.npy\")\n",
    "    d9 = np.load(\"mnist_nn_experiments_scrambled_run9/val_error_scrambled.npy\")\n",
    "    d = np.array([d0,d1,d2,d3,d4,d5,d6,d7,d8,d9])\n",
    "    \n",
    "    print ttest_ind(c[:,-1],d[:,-1])\n",
    "\n",
    "    plt.plot(smooth(c.mean(axis=0),53,\"flat\"), color=\"k\", linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(smooth(d.mean(axis=0),53,\"flat\"), color=\"r\", linewidth=1, linestyle=\"-\")\n",
    "    plt.xlabel(\"Epochs\", fontsize=16)\n",
    "    plt.ylabel(\"Test Error\", fontsize=16)\n",
    "    plt.ylim((0.04,0.055))\n",
    "    plt.xlim((75,4000))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"mnist_nn_experiments_scrambled_plot.png\", type=\"png\", dpi=600)\n",
    "    plt.savefig(\"mnist_nn_experiments_scrambled_plot.pdf\", type=\"pdf\", dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
