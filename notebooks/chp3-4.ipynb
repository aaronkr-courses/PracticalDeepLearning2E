{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817d8c5e",
   "metadata": {},
   "source": [
    "# Chp 3 Introduction to ML\n",
    "\n",
    "Decision tree construction relies on recursion, where a function calls itself on smaller versions of the problem until reaching a stopping condition. For example, the factorial function can be defined recursively:\n",
    "\n",
    "```python\n",
    "𝑛! = 𝑛 × (𝑛−1)!\n",
    "```\n",
    "\n",
    "In the same way, decision trees build subtrees by applying the same process recursively on subsets of the training data, until reaching a leaf node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorial\n",
    "def fact(n):\n",
    "    if (n <= 1):\n",
    "        return 1\n",
    "    else:\n",
    "        return n*fact(n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2602f069",
   "metadata": {},
   "source": [
    "# Chp 4 Experiments with Classical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6703acb",
   "metadata": {},
   "source": [
    "## Iris\n",
    "\n",
    "The iris dataset contains four continuous features—sepal length, sepal width, petal length, and petal width—and three classes corresponding to iris species. It has 150 samples, 50 per class. Using PCA augmentation, we expand the dataset to 1,200 training samples while keeping the same test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c923a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris Experiments\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"    predictions  :\", clf.predict(x_test))\n",
    "    print(\"    actual labels:\", y_test)\n",
    "    print(\"    score = %0.4f\" % clf.score(x_test, y_test))\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"../data/iris/iris_features.npy\")\n",
    "    y = np.load(\"../data/iris/iris_labels.npy\")\n",
    "    N = 120 \n",
    "    x_train = x[:N]; x_test = x[N:]\n",
    "    y_train = y[:N]; y_test = y[N:]\n",
    "    xa_train=np.load(\"../data/iris/iris_train_features_augmented.npy\")\n",
    "    ya_train=np.load(\"../data/iris/iris_train_labels_augmented.npy\")\n",
    "    xa_test =np.load(\"../data/iris/iris_test_features_augmented.npy\")\n",
    "    ya_test =np.load(\"../data/iris/iris_test_labels_augmented.npy\")\n",
    "\n",
    "    print(\"Nearest centroid:\")\n",
    "    run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "    print(\"k-NN classifier (k=3):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "    print(\"Naive Bayes classifier (Gaussian):\")\n",
    "    run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "    print(\"Naive Bayes classifier (Multinomial):\")\n",
    "    run(x_train, y_train, x_test, y_test, MultinomialNB())\n",
    "    print(\"Decision tree classifier:\")\n",
    "    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "    print(\"Random forest classifier (estimators=5):\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, RandomForestClassifier(n_estimators=5))\n",
    "\n",
    "    print(\"SVM (linear, C=1.0):\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, SVC(kernel=\"linear\", C=1.0))\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.25):\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.25))\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.001, augmented)\")\n",
    "    run(xa_train, ya_train, xa_test, ya_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.001))\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.001, original)\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.001))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c231fbb",
   "metadata": {},
   "source": [
    "### Implementing a Nearest-Centroid Classifier\n",
    "\n",
    "Even without sklearn, we can quickly implement a nearest-centroid classifier for the iris dataset. The process involves calculating the per-feature means (centroids) of each class from the training samples. This is all that is needed to \"train\" the model. Predictions are made by computing the Euclidean distance from each test sample to the three centroids, assigning the sample to the class with the nearest centroid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def centroids(x,y):\n",
    "    c0 = x[np.where(y==0)].mean(axis=0)\n",
    "    c1 = x[np.where(y==1)].mean(axis=0)\n",
    "    c2 = x[np.where(y==2)].mean(axis=0)\n",
    "    return [c0,c1,c2]\n",
    "\n",
    "def predict(c0,c1,c2,x):\n",
    "    p = np.zeros(x.shape[0], dtype=\"uint8\")\n",
    "    for i in range(x.shape[0]):\n",
    "        d = [((c0-x[i])**2).sum(),\n",
    "             ((c1-x[i])**2).sum(),\n",
    "             ((c2-x[i])**2).sum()]\n",
    "        p[i] = np.argmin(d)\n",
    "    return p\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"../data/iris/iris_features.npy\")\n",
    "    y = np.load(\"../data/iris/iris_labels.npy\")\n",
    "    N = 120\n",
    "    x_train = x[:N]; x_test = x[N:]\n",
    "    y_train = y[:N]; y_test = y[N:]\n",
    "    c0, c1, c2 = centroids(x_train, y_train)\n",
    "    p = predict(c0,c1,c2, x_test)\n",
    "    nc = len(np.where(p == y_test)[0])\n",
    "    nw = len(np.where(p != y_test)[0])\n",
    "    acc = float(nc) / (float(nc)+float(nw))\n",
    "    print(\"predicted:\", p)\n",
    "    print(\"actual   :\", y_test)\n",
    "    print(\"test accuracy = %0.4f\" % acc)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059011e",
   "metadata": {},
   "source": [
    "## Breast Cancer\n",
    "\n",
    "The breast cancer dataset contains 569 samples, each with 30 continuous features, including 212 malignant and 357 benign cases. Before training, we normalize the dataset by subtracting the mean and dividing by the standard deviation for each feature. Normalization ensures all features are on a similar scale, improving performance for many models.\n",
    "\n",
    "Using an 80/20 train-test split (455 training samples and 114 test samples), we train nine classifiers: nearest centroid, k-NN, naive Bayes, decision tree, random forest (two variants), linear SVM, and RBF SVM. For the SVMs, we set the margin constant C to the default 1.0, and γ for the RBF kernel to 0.0333 (1/30).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC experiements\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"    score = %0.4f\" % clf.score(x_test, y_test))\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"../data/breast/bc_features_standard.npy\")\n",
    "    y = np.load(\"../data/breast/bc_labels.npy\")\n",
    "    N = 455 \n",
    "    x_train = x[:N];  x_test = x[N:]\n",
    "    y_train = y[:N];  y_test = y[N:]\n",
    "\n",
    "    print(\"Nearest centroid:\")\n",
    "    run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "    print(\"k-NN classifier (k=3):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "    print(\"k-NN classifier (k=7):\")\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "    print(\"Naive Bayes classifier (Gaussian):\")\n",
    "    run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "    print(\"Decision tree classifier:\")\n",
    "    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "    print(\"Random forest classifier (estimators=5):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "    print(\"Random forest classifier (estimators=50):\")\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "    print(\"SVM (linear, C=1.0):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"linear\", C=1.0))\n",
    "    print(\"SVM (RBF, C=1.0, gamma=0.03333):\")\n",
    "    run(x_train, y_train, x_test, y_test, SVC(kernel=\"rbf\", C=1.0, gamma=0.03333))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ee989",
   "metadata": {},
   "source": [
    "### Adding k-Fold Validation\n",
    "\n",
    "To implement k-fold validation, we first select a value for k. For the breast cancer dataset with 569 samples, a balance is needed: smaller k ensures each fold has enough samples to represent the data reasonably, while larger k helps average out the effects of a “bad” split. A common choice is k = 5, giving roughly 113 samples per fold, with 80% for training and 20% for testing. The code is designed to allow easy adjustment of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e312fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC K-Fold\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import sys\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n",
    "\n",
    "def split(x,y,k,m):\n",
    "    ns = int(y.shape[0]/m)\n",
    "    s = []\n",
    "    for i in range(m):\n",
    "    \ts.append([x[(ns*i):(ns*i+ns)],\n",
    "                  y[(ns*i):(ns*i+ns)]])\n",
    "    x_test, y_test = s[k]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(m):\n",
    "        if (i==k):\n",
    "            continue\n",
    "        else:\n",
    "            a,b = s[i]\n",
    "            x_train.append(a)\n",
    "            y_train.append(b)\n",
    "    x_train = np.array(x_train).reshape(((m-1)*ns,30))\n",
    "    y_train = np.array(y_train).reshape((m-1)*ns)\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def pp(z,k,s):\n",
    "    m = z.shape[1]\n",
    "    print(\"%-19s: %0.4f +/- %0.4f | \" % (s, z[k].mean(), z[k].std()/np.sqrt(m)), end='')\n",
    "    for i in range(m):\n",
    "        print(\"%0.4f \" % z[k,i], end='')\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    x = np.load(\"../data/breast/bc_features_standard.npy\")\n",
    "    y = np.load(\"../data/breast/bc_labels.npy\")\n",
    "    idx = np.argsort(np.random.random(y.shape[0]))\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "    m = int(sys.argv[1])\n",
    "    z = np.zeros((8,m))\n",
    "\n",
    "    for k in range(m):\n",
    "        x_train, y_train, x_test, y_test = split(x,y,k,m)\n",
    "        z[0,k] = run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "        z[1,k] = run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "        z[2,k] = run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "        z[3,k] = run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "        z[4,k] = run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "        z[5,k] = run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "        z[6,k] = run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "        z[7,k] = run(x_train, y_train, x_test, y_test, SVC(kernel=\"linear\", C=1.0))\n",
    "\n",
    "    pp(z,0,\"Nearest\"); pp(z,1,\"3-NN\")\n",
    "    pp(z,2,\"7-NN\");    pp(z,3,\"Naive Bayes\")\n",
    "    pp(z,4,\"Decision tree\");    pp(z,5,\"Random forest (5)\")\n",
    "    pp(z,6,\"Random forest (50)\");    pp(z,7,\"SVM (linear)\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f8156",
   "metadata": {},
   "source": [
    "### Fine-Tuning the RBF Kernel SVM\n",
    "\n",
    "For the RBF (Gaussian) kernel SVM, both C and γ must be optimized. A 2D grid search is performed:\n",
    "\n",
    "- C uses the same range as the linear SVM.\n",
    "- γ is selected from powers of two times the default 1/30, for p ∈ [–4, 3].\n",
    "\n",
    "For each pair (C, γ), five-fold validation is performed, and the pair with the highest mean accuracy is selected. Repeated runs produce slightly different results due to randomization in the dataset ordering.\n",
    "\n",
    "One promising combination is (C, γ) = (10, 0.00417), which achieves a grand mean accuracy of 97.70%, the highest among all models tested on the breast cancer dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC RBF SVM Search\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n",
    "\n",
    "def split(x,y,k,m):\n",
    "    ns = int(y.shape[0]/m)\n",
    "    s = []\n",
    "    for i in range(m):\n",
    "        s.append([x[(ns*i):(ns*i+ns)], y[(ns*i):(ns*i+ns)]])\n",
    "    x_test, y_test = s[k]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(m):\n",
    "        if (i==k):\n",
    "            continue\n",
    "        else:\n",
    "            a,b = s[i]\n",
    "            x_train.append(a)\n",
    "            y_train.append(b)\n",
    "    x_train = np.array(x_train).reshape(((m-1)*ns,30))\n",
    "    y_train = np.array(y_train).reshape((m-1)*ns)\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def main():\n",
    "    m = 5 \n",
    "    x = np.load(\"../data/breast/bc_features_standard.npy\")\n",
    "    y = np.load(\"../data/breast/bc_labels.npy\")\n",
    "    idx = np.argsort(np.random.random(y.shape[0]))\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    Cs = np.array([0.01,0.1,1.0,2.0,10.0,50.0,100.0])\n",
    "    gs = (1./30)*2.0**np.array([-4,-3,-2,-1,0,1,2,3])\n",
    "    zmax = 0.0 \n",
    "    for C in Cs: \n",
    "        for g in gs: \n",
    "            z = np.zeros(m)\n",
    "            for k in range(m):\n",
    "                x_train, y_train, x_test, y_test = split(x,y,k,m)\n",
    "                z[k] = run(x_train, y_train, x_test, y_test, SVC(C=C,gamma=g,kernel=\"rbf\"))\n",
    "            if (z.mean() > zmax):\n",
    "                zmax = z.mean()\n",
    "                bestC = C \n",
    "                bestg = g \n",
    "    print(\"best C     = %0.5f\" % bestC)\n",
    "    print(\"     gamma = %0.5f\" % bestg)\n",
    "    print(\"   accuracy= %0.5f\" % zmax)\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed553cb4",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "The final dataset examined in this chapter is the vector version of MNIST, which contains 28×28 grayscale images of handwritten digits (0–9), one per image. MNIST is a foundational dataset in machine learning and deep learning and will be used throughout the book.\n",
    "\n",
    "MNIST has 60,000 training images and 10,000 test images, roughly balanced across the 10 digits. Because the dataset is large, classical models are trained directly on the training set and tested on the test set, without using k-fold validation.\n",
    "\n",
    "The images are converted into vectors of 784 elements (28 × 28 pixels), with values from 0 to 255. Three versions of the dataset are considered:\n",
    "\n",
    "1. Raw byte values (0–255)\n",
    "2. Scaled data to [0, 1) by dividing by 256\n",
    "3. Normalized data, where each pixel has its mean subtracted and is divided by its standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df99a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST experiments\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import decomposition\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    s = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    e_train = time.time() - s \n",
    "    s = time.time()\n",
    "    score = clf.score(x_test, y_test)\n",
    "    e_test = time.time() - s \n",
    "    print(\"score = %0.4f (time, train=%8.3f, test=%8.3f)\" % (score, e_train, e_test))\n",
    "\n",
    "def train(x_train, y_train, x_test, y_test):\n",
    "    print(\"    Nearest centroid          : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, NearestCentroid())\n",
    "    print(\"    k-NN classifier (k=3)     : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))\n",
    "    print(\"    k-NN classifier (k=7)     : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))\n",
    "    print(\"    Naive Bayes (Gaussian)    : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, GaussianNB())\n",
    "    print(\"    Decision tree             : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())\n",
    "    print(\"    Random forest (trees=  5) : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=5))\n",
    "    print(\"    Random forest (trees= 50) : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=50))\n",
    "    print(\"    Random forest (trees=500) : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=500))\n",
    "    print(\"    Random forest (trees=1000): \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, RandomForestClassifier(n_estimators=1000))\n",
    "    print(\"    LinearSVM (C=0.01)        : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, LinearSVC(C=0.01))\n",
    "    print(\"    LinearSVM (C=0.1)         : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, LinearSVC(C=0.1))\n",
    "    print(\"    LinearSVM (C=1.0)         : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, LinearSVC(C=1.0))\n",
    "    print(\"    LinearSVM (C=10.0)        : \", end='')\n",
    "    run(x_train, y_train, x_test, y_test, LinearSVC(C=10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b91178",
   "metadata": {},
   "source": [
    "The code uses LinearSVC instead of SVC for runtime efficiency and multiclass handling. Helper functions track both model accuracy and training/testing time, important due to the dataset’s larger size. Training is repeated for the raw, scaled, and normalized versions of the dataset.\n",
    "\n",
    "Normalization uses the training set’s mean and standard deviation, which are also applied to test data, as these better represent the true distribution. PCA is also applied, reducing the 784 features to 15 principal components, capturing just over 33% of the variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "\n",
    "    print(\"Models trained on raw [0,255] images:\")\n",
    "    train(x_train, y_train, x_test, y_test)\n",
    "    print(\"Models trained on raw [0,1) images:\")\n",
    "    train(x_train/256.0, y_train, x_test/256.0, y_test)\n",
    "\n",
    "    m = x_train.mean(axis=0)\n",
    "    s = x_train.std(axis=0) + 1e-8\n",
    "    x_ntrain = (x_train - m) / s\n",
    "    x_ntest  = (x_test - m) / s\n",
    "\n",
    "    print(\"Models trained on normalized images:\")\n",
    "    train(x_ntrain, y_train, x_ntest, y_test)\n",
    "\n",
    "    pca = decomposition.PCA(n_components=15)\n",
    "    pca.fit(x_ntrain)\n",
    "    x_ptrain = pca.transform(x_ntrain)\n",
    "    x_ptest = pca.transform(x_ntest)\n",
    "    \n",
    "    print(\"Models trained on first 15 PCA components of normalized images:\")\n",
    "    train(x_ptrain, y_train, x_ptest, y_test)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69bd101",
   "metadata": {},
   "source": [
    "### Experimenting with PCA Components\n",
    "\n",
    "Previously, 15 PCA components were used, representing about 33% of the dataset’s variance. To explore the effect of PCA further, the number of components is varied from 10 to 780, and three models are trained for each setting: Gaussian naive Bayes, random forest (50 trees), and linear SVM (C = 1.0). This process is computationally intensive and took over 10 hours on a low-end machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import decomposition\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf):\n",
    "    s = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    e_train = time.time() - s \n",
    "    s = time.time()\n",
    "    score = clf.score(x_test, y_test)\n",
    "    e_test = time.time() - s \n",
    "    return [score, e_train, e_test]\n",
    "\n",
    "def main():\n",
    "    x_train = np.load(\"../data/mnist/mnist_train_vectors.npy\").astype(\"float64\")\n",
    "    y_train = np.load(\"../data/mnist/mnist_train_labels.npy\")\n",
    "    x_test = np.load(\"../data/mnist/mnist_test_vectors.npy\").astype(\"float64\")\n",
    "    y_test = np.load(\"../data/mnist/mnist_test_labels.npy\")\n",
    "    m = x_train.mean(axis=0)\n",
    "    s = x_train.std(axis=0) + 1e-8\n",
    "    x_ntrain = (x_train - m) / s \n",
    "    x_ntest  = (x_test - m) / s \n",
    "\n",
    "    n = 78\n",
    "    pcomp = np.linspace(10,780,n, dtype=\"int16\")\n",
    "    nb=np.zeros((n,4))\n",
    "    rf=np.zeros((n,4))\n",
    "    sv=np.zeros((n,4))\n",
    "    tv=np.zeros((n,2))\n",
    "\n",
    "    for i,p in enumerate(pcomp):\n",
    "        pca = decomposition.PCA(n_components=p)\n",
    "        pca.fit(x_ntrain)\n",
    "        xtrain = pca.transform(x_ntrain)\n",
    "        xtest = pca.transform(x_ntest)\n",
    "        tv[i,:] = [p, pca.explained_variance_ratio_.sum()]\n",
    "        sc,etrn,etst =run(xtrain, y_train, xtest, y_test, GaussianNB())\n",
    "        nb[i,:] = [p,sc,etrn,etst]\n",
    "        sc,etrn,etst =run(xtrain, y_train, xtest, y_test, RandomForestClassifier(n_estimators=50))\n",
    "        rf[i,:] = [p,sc,etrn,etst]\n",
    "        sc,etrn,etst =run(xtrain, y_train, xtest, y_test, LinearSVC(C=1.0))\n",
    "        sv[i,:] = [p,sc,etrn,etst]\n",
    "\n",
    "    np.save(\"../data/mnist/mnist_pca_tv.npy\", tv) \n",
    "    np.save(\"../data/mnist/mnist_pca_nb.npy\", nb)\n",
    "    np.save(\"../data/mnist/mnist_pca_rf.npy\", rf)\n",
    "    np.save(\"../data/mnist/mnist_pca_sv.npy\", sv)\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb0b1c",
   "metadata": {},
   "source": [
    "## Classical Model Summary\n",
    "\n",
    "The chapter concludes with a summary of pros and cons for the six classical models discussed:\n",
    "\n",
    "### Nearest Centroid\n",
    "\n",
    "- **Pros:** Simple implementation, fast training, low memory use, supports multiclass classification, fast inference.\n",
    "- **Cons:** Assumes each class forms a tight cluster in feature space, often too simplistic for complex data. Variants with multiple centroids per class can improve performance.\n",
    "\n",
    "### k-Nearest Neighbors (k-NN)\n",
    "\n",
    "- **Pros:** No explicit training required, works well with large datasets, supports multiclass classification naturally.\n",
    "- **Cons:** Slow inference because distances must be computed for every training sample, even with optimized algorithms.\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "- **Pros:** Fast to train and classify, supports multiclass problems, works for both discrete and continuous features.\n",
    "- **Cons:** Assumes feature independence, which is rarely true in practice. Continuous features often require additional distributional assumptions (e.g., Gaussian).\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "- **Pros:** Fast training and inference, interpretable, supports multiclass and mixed feature types, can justify decisions with a clear path from root to leaf.\n",
    "- **Cons:** Prone to overfitting, interpretability decreases with tree size, requires balancing tree depth against accuracy.\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "- **Pros:** Robust to overfitting, supports multiclass problems, reasonably fast to train and infer, less sensitive to feature scaling, accuracy improves with more trees.\n",
    "- **Cons:** Harder to interpret than single decision trees, inference time scales linearly with the number of trees, stochastic performance can vary slightly between trainings.\n",
    "\n",
    "### Support Vector Machines (SVMs)\n",
    "\n",
    "- **Pros:** Can achieve excellent performance, fast inference after training.\n",
    "- **Cons:** Multiclass requires multiple models, only supports continuous features, sensitive to feature scaling, difficult to train on large datasets with non-linear kernels, requires careful hyperparameter tuning.\n",
    "\n",
    "## When to Use Classical Models\n",
    "\n",
    "Classical models remain appropriate under certain conditions:\n",
    "\n",
    "1. **Small datasets:** They perform well when there are only tens or hundreds of examples, unlike deep learning models that require larger datasets.\n",
    "2. **Limited computational resources:** Simple models (nearest centroid, naive Bayes, decision trees, SVMs) are feasible on low-power devices; k-NN may be too slow unless the dataset is small.\n",
    "3. **Explainability:** Models like decision trees, k-NN, nearest centroid, and naive Bayes can explain their predictions, unlike deep neural networks.\n",
    "4. **Vector inputs without structure:** When features are independent and unstructured (not spatially correlated as in images), classical models are suitable.\n",
    "\n",
    "These are rules of thumb, not hard rules. Deep learning could be used even when these conditions apply, but classic models may provide sufficient performance with less complexity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21f07c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
